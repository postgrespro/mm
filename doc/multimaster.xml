<sect1 id="multimaster" xreflabel="multimaster">
  <title>multimaster</title>
  <para>
    <filename>multimaster</filename> is a <productname>PostgreSQL</productname> extension with a set
    of patches that turns <productname>PostgreSQL</productname> into a synchronous shared-nothing
    cluster to provide Online Transaction Processing (<acronym>OLTP</acronym>) scalability for read transactions and high availability with automatic disaster recovery.</para>
    <para> As compared to a standard <productname>PostgreSQL</productname> master-standby cluster, a cluster configured with the <filename>multimaster</filename> extension offers the following benefits:</para>
    <itemizedlist>
      <listitem>
        <para>
          Fault tolerance and automatic node recovery
        </para>
      </listitem>
      <listitem>
        <para>
          Synchronous logical replication and DDL replication
        </para>
      </listitem>
      <listitem>
        <para>
          Read scalability
        </para>
      </listitem>
      <listitem>
        <para>
         Working with temporary tables on each cluster node
        </para>
      </listitem>
      <listitem>
        <para>
        <productname>PostgreSQL</productname> online upgrades
        </para>
      </listitem>
    </itemizedlist>
    <important>
     <para>
      Before deploying <filename>multimaster</filename> on production
      systems, make sure to take its replication restrictions into
      account. For details, see <xref linkend="multimaster-limitations"/>.
     </para>
    </important>
    <para>
      The <filename>multimaster</filename> extension replicates your
      database to all nodes of the cluster and allows write transactions
      on each node. Write transactions are synchronously replicated to all nodes,
      which increases commit latency. Read-only transactions and queries
      are executed locally, without any measurable overhead.
    </para>
    <para>
      To ensure high availability and fault tolerance of the cluster,
      <filename>multimaster</filename> determines each transaction outcome through Paxos consensus algorithm,
      uses custom recovery protocol
      and heartbeats for failure discovery. A multi-master cluster of <replaceable>N</replaceable>
      nodes can continue working while the majority of the nodes are
      alive and reachable by other nodes. To be configured with
      <filename>multimaster</filename>, the cluster must include at least
      two nodes. Since the data on all cluster nodes is the same, you do not
      typically need more than five cluster nodes. Three cluster nodes are
      enough to ensure high availability in most cases.
      There is also a special 2+1 (referee) mode in which 2 nodes hold data and
      an additional one called <firstterm>referee</firstterm> only participates in voting. Compared to traditional three
      nodes setup, this is cheaper (referee resources demands are low) but availability
      is decreased. For details, see <xref linkend="setting-up-a-referee"/>.
    </para>
      <para>When a failed node
      is reconnected to the cluster, <filename>multimaster</filename> automatically
      fast-forwards the node to the actual state based on the
      Write-Ahead Log (<acronym>WAL</acronym>) data in the corresponding replication slot.
      If a node was excluded from the cluster, you can <link linkend="multimaster-adding-new-nodes-to-the-cluster">add it back using <application>pg_basebackup</application></link>.
    </para>
    <para>
      To learn more about the <filename>multimaster</filename> internals, see
      <xref linkend="multimaster-architecture"/>.
    </para>

  <sect2 id="multimaster-limitations">
    <title>Limitations</title>
    <para>The <filename>multimaster</filename> extension takes care of the database replication in a fully automated way. You can perform write transactions on any node and work with temporary tables on each cluster node simultaneously. However, make sure to take the following replication restrictions into account:</para>
    <itemizedlist>
      <listitem>
        <para>
          Microsoft Windows operating system is not supported.
        </para>
      </listitem>
      <listitem>
        <para>
          1C solutions are not supported.
        </para>
      </listitem>
      <listitem>
        <para>
          <filename>multimaster</filename> can replicate only one database
          in a cluster. If it is required to replicate the contents of several
          databases, you can either transfer all data into different schemas
          within a single database or create a separate cluster for each
          database and set up <filename>multimaster</filename> for each cluster.
        </para>
      </listitem>
      <listitem>
        <para>
          <ulink url="https://postgrespro.com/docs/postgresql/current/lo">Large objects</ulink> are not supported. Although creating large objects is
          allowed, <application>multimaster</application> cannot replicate such
          objects, and their OIDs may conflict on different nodes, so their use
          is not recommended.
        </para>
      </listitem>
      <listitem>
        <para>
          Since <filename>multimaster</filename> is based on
          <link linkend="multimaster-architecture">logical replication
          and Paxos over two-phase commit protocol</link>, its operation is
          highly affected by network latency. It is not recommended to
          set up a <filename>multimaster</filename> cluster with geographically
          distributed nodes.
        </para>
      </listitem>
      <listitem>
        <para>
          Using tables without primary keys can have negative impact
          on performance. In some cases, it can even lead to inability
          to restore a cluster node, so you should avoid replicating such
          tables with <filename>multimaster</filename>.
        </para>
      </listitem>
      <listitem>
        <para>
          Unlike in vanilla <productname>PostgreSQL</productname>, <literal>read committed</literal>
          isolation level can cause serialization failures on a multi-master cluster (with an SQLSTATE code '40001') if there are
          conflicting transactions from different nodes, so the application must be
          ready to retry transactions.
          <emphasis><literal>Serializable</literal></emphasis> isolation level works
          only with respect to local transactions on the current node.
        </para>
      </listitem>
      <listitem>
        <para>
          Sequence generation. To avoid conflicts between unique identifiers on different nodes,
          <filename>multimaster</filename> modifies the default behavior of sequence generators.
          By default, ID generation on each node is started with this node number and is
          incremented by the number of nodes. For example, in a three-node cluster, 1, 4, and 7
          IDs are allocated to the objects written onto the first node, while 2, 5, and 8 IDs are reserved
          for the second node. If you change the number of nodes in the cluster, the incrementation
          interval for new IDs is adjusted accordingly. Thus, the generated sequence values are not
          monotonic. If it is critical to get a monotonically increasing sequence cluster-wide, you can
          set the <link linkend="mtm-monotonic-sequences"><varname>multimaster.monotonic_sequences</varname></link>
          to <literal>true</literal>.
        </para>
      </listitem>
      <listitem>
        <para>
          Commit latency. In the current implementation of logical
          replication, <filename>multimaster</filename> sends data to subscriber nodes only after the
          local commit, so you have to wait for transaction processing twice: first on the local node,
          and then on all the other nodes simultaneously. In the case of a heavy-write transaction,
          this may result in a noticeable delay.
        </para>
      </listitem>
      <listitem>
        <para>
          Logical replication does not guarantee that a system object
          OID is the same on all cluster nodes, so OIDs for the same object may
          differ between <filename>multimaster</filename> cluster nodes.
          If your driver or application relies on OIDs, make sure that their
          use is restricted to connections to one and the same node to avoid
          errors. For example, the <filename>Npgsql</filename> driver may not
          work correctly with <filename>multimaster</filename> if the
          <literal>NpgsqlConnection.GlobalTypeMapper</literal> method tries
          using OIDs in connections to different cluster nodes.
        </para>
      </listitem>
      <listitem>
        <para>
          Replicated non-conflicting transactions are applied on the receiving nodes
          in parallel, so such transactions may become visible on different nodes in different order.
        </para>
      </listitem>
      <listitem>
        <para>
         <literal>CREATE INDEX CONCURRENTLY</literal> and <literal>REINDEX
          CONCURRENTLY</literal> are not supported.
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>COMMIT AND CHAIN</literal> feature is not supported.
        </para>
      </listitem>
    </itemizedlist>
  </sect2>

  <sect2 id="multimaster-architecture">
  <title>Architecture</title>
  <sect3 id="multimaster-replication">
    <title>Replication</title>
    <para>
      Since each server in a multi-master cluster can accept writes, any server can abort a
      transaction because of a concurrent update &mdash; in the same way as it
      happens on a single server between different backends. To ensure
      high availability and data consistency on all cluster nodes,
      <filename>multimaster</filename> uses
      <ulink url="https://postgrespro.com/docs/postgresql/current/logicaldecoding-synchronous">logical replication</ulink>
      and the two phase commit protocol with transaction outcome determined by
      <link linkend="multimaster-credits">Paxos consensus algorithm</link>.
    </para>
    <para>
      When <productname>PostgreSQL</productname> loads the <filename>multimaster</filename> shared
      library, <filename>multimaster</filename> sets up a logical
      replication producer and consumer for each node, and hooks into
      the transaction commit pipeline. The typical data replication
      workflow consists of the following phases:
    </para>
    <orderedlist>
      <listitem>
        <para>
          <literal>PREPARE</literal> phase.
          <filename>multimaster</filename> captures and implicitly
          transforms each <literal>COMMIT</literal> statement to a
          <literal>PREPARE</literal> statement. All the nodes that get
          the transaction via the replication protocol (<emphasis>the
          cohort nodes</emphasis>) send their vote for approving or
          declining the transaction to the backend process on the
          initiating node. This ensures that all the cohort can accept
          the transaction, and no write conflicts occur. For details on
          <literal>PREPARE</literal> transactions support in <productname>PostgreSQL</productname>,
          see the
	  <ulink url="https://postgrespro.com/docs/postgresql/current/sql-prepare-transaction">
          PREPARE TRANSACTION</ulink> topic.
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>PRECOMMIT</literal> phase. If all the cohort nodes approve
          the transaction, the backend process sends a
          <literal>PRECOMMIT</literal> message to all the cohort nodes
          to express an intention to commit the transaction. The cohort
          nodes respond to the backend with the
          <literal>PRECOMMITTED</literal> message. In case of a failure,
          all the nodes can use this information to complete the
          transaction using a quorum-based voting procedure.
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>COMMIT</literal> phase. If
          <literal>PRECOMMIT</literal> is successful, the transaction
          is committed to all nodes.
        </para>
      </listitem>
    </orderedlist>

    <para>
      If a node crashes or gets disconnected from the cluster between
      the <literal>PREPARE</literal> and <literal>COMMIT</literal>
      phases, the <literal>PRECOMMIT</literal> phase ensures that the
      survived nodes have enough information to complete the prepared
      transaction. The <literal>PRECOMMITTED</literal> messages help
      avoid the situation when the crashed node has already committed
      or aborted the transaction, but has not notified other nodes
      about the transaction status. In a two-phase commit (2PC), such a
      transaction would block resources (hold locks) until the recovery
      of the crashed node. Otherwise, data inconsistencies can appear
      in the database when the failed node is recovered, for example, if
      the failed node committed the transaction, but the survived node
      aborted it.
    </para>
    <para>
      To complete the transaction, the backend must receive a response
      from the majority of the nodes. For example, for a cluster of 2<replaceable>N</replaceable>+1 nodes,
      at least <replaceable>N</replaceable>+1 responses are required. Thus, <filename>multimaster</filename> ensures that
      your cluster is available for reads and writes while the majority
      of the nodes are connected, and no data inconsistencies occur in
      case of a node or connection failure.
    </para>
  </sect3>
  <sect3 id="multimaster-failure-detection-and-recovery">
    <title>Failure Detection and Recovery</title>
    <para>
      Since <filename>multimaster</filename> allows writes to each node,
      it has to wait for responses about transaction acknowledgment
      from all the other nodes. Without special actions in case of a
      node failure, each commit would have to wait until the failed node
      recovery. To deal with such situations,
      <filename>multimaster</filename> periodically sends heartbeats to
      check the node state and the connectivity between nodes. When several
      heartbeats to the node are lost in a row, this node is kicked out
      of the cluster to allow writes to the remaining alive nodes. You
      can configure the heartbeat frequency and the response timeout in
      the <varname>multimaster.heartbeat_send_timeout</varname> and
      <varname>multimaster.heartbeat_recv_timeout</varname> parameters,
      respectively.
    </para>
    <para>
      For example, suppose a five-node multi-master cluster experienced
      a network failure that split the network into two isolated
      subnets, with two and three cluster nodes. Based on heartbeats
      propagation information, <filename>multimaster</filename> will
      continue accepting writes at each node in the bigger partition,
      and deny all writes in the smaller one. Thus, a cluster consisting
      of 2<replaceable>N</replaceable>+1 nodes can tolerate <replaceable>N</replaceable> node failures and stay alive if any
      <replaceable>N</replaceable>+1 nodes are alive and connected to each other.
      You can also set up a two nodes cluster plus a
      lightweight referee node that does not hold the data, but acts as
      a tie-breaker during symmetric node partitioning. For details,
      see <xref linkend="setting-up-a-referee"/>.
    </para>

    <para>
      In case of a partial network split when different nodes have
      different connectivity, <filename>multimaster</filename> finds a
      fully connected subset of nodes and disconnects nodes outside of this subset. For
      example, in a three-node cluster, if node A can access both B and
      C, but node B cannot access node C, <filename>multimaster</filename>
      isolates node C to ensure that both A and B can work.
    </para>

    <para>
      To preserve order of transactions on different nodes and thus data
      integrity, the decision to exclude or add back node(s) must be taken
      coherently. Generations which represent a subset of
      currently supposedly live nodes serve this
      purpose. Technically, generation is a pair <literal>&lt;n, members&gt;</literal>
      where <replaceable>n</replaceable> is unique number and
      <replaceable>members</replaceable> is subset of configured nodes. A node always
      lives in some generation and switches to the one with higher number as soon
      as it learns about its existence; generation numbers act as logical
      clocks/terms/epochs here. Each transaction is stamped during commit with
      current generation of the node it is being executed on. The transaction
      can be proposed to be committed only after it has been PREPAREd on all its
      generation members. This allows to design the recovery protocol so that
      order of conflicting committed transactions is the same on all nodes. Node
      resides in generation in one of three states (can be shown with <literal>mtm.status()</literal>):
      <orderedlist>
	<listitem>
	  <para><literal>ONLINE</literal>: node is member of the generation and
	  making transactions normally;</para>
	</listitem>
	<listitem>
	  <para><literal>RECOVERY</literal>: node is member of the generation, but it
	  must apply in recovery mode transactions from previous generations to become <literal>ONLINE</literal>;</para>
	</listitem>
	<listitem>
	  <para><literal>DEAD</literal>: node will never be <filename>ONLINE</filename> in this generation;</para>
	</listitem>
      </orderedlist>

    </para>

    <para>
      For alive nodes, there is no way to distinguish between a failed
      node that stopped serving requests and a network-partitioned node
      that can be accessed by database users, but is unreachable for
      other nodes. If during commit of writing transaction some of current generation members are disconnected,
      transaction is rolled back according to generation rules. To avoid futile work,
      connectivity is also checked during transaction start; if
      you try to access an isolated node, <filename>multimaster</filename>
      returns an error message indicating the current status of the node.
      Thus, to prevent stale reads read-only queries are also forbidden.
      If you would like to continue using a disconnected node outside of
      the cluster in the standalone mode, you have to uninstall the
      <filename>multimaster</filename> extension on this node, as
      explained in <xref linkend="uninstalling-multimaster-extension"/>.
    </para>

    <para>
      Each node maintains a data structure that keeps the information about the state of all
      nodes in relation to this node. You can get this data by calling the
      <literal>mtm.status()</literal> and the <literal>mtm.nodes()</literal> functions.
    </para>
    <para>
      When a failed node connects back to the cluster,
      <filename>multimaster</filename> starts automatic recovery:
    </para>
    <orderedlist>
      <listitem>
        <para>
          The reconnected node selects a cluster node which is
          <literal>ONLINE</literal> in the highest generation and starts
          catching up with the current state of the cluster based on the
          Write-Ahead Log (WAL).
        </para>
      </listitem>
      <listitem>
	<para>
	  When the node is caught up, it ballots for including itself in the next
	  generation. Once generation is elected, commit of new transactions
	  will start waiting for apply on the joining node.
	</para>
      </listitem>
      <listitem>
        <para>
          When the rest of transactions till the switch to the new generation is
          applied, the reconnected node is
          promoted to the <literal>online</literal> state and included into
          the replication scheme.
        </para>
      </listitem>
    </orderedlist>

    <para>
    The correctness of recovery protocol was verified with TLA+ model
    checker. You can find the model (and more detailed description) at
    <filename>doc/specs</filename> directory of the source distribution.
    </para>

    <para>
      Automatic recovery requires presence of all WAL files generated after node
      failure. If a node is down for a long time and storing more WALs is
      unacceptable, you may have to exclude this node from the cluster and
      manually restore it from one of the working nodes using
      <application>pg_basebackup</application>. For details, see <xref
      linkend="multimaster-adding-new-nodes-to-the-cluster"/>.
    </para>
  </sect3>

  <sect3 id="multimaster-bgworkers">
    <title>Multimaster Background Workers</title>
    <variablelist>
      <varlistentry id="mtm-monitor">
        <term>mtm-monitor</term>
        <listitem>
        <para>
          Starts all other workers for a database managed with <application>multimaster</application>.
          This is the first worker loaded during <application>multimaster</application> boot.
          Each <application>multimaster</application> node has a single <literal>mtm-monitor</literal> worker.
          When a new node is added, <literal>mtm-monitor</literal> starts <literal>mtm-logrep-receiver</literal> and
          <literal>mtm-dmq-receiver</literal> workers to enable replication to this node.
          If a node is dropped, <literal>mtm-monitor</literal> stops <literal>mtm-logrep-receiver</literal>
          and <literal>mtm-dmq-receiver</literal> workers that have been serving the dropped node.
          Each <literal>mtm-monitor</literal> controls workers on its own node only.
        </para>
        </listitem>
      </varlistentry>
      <varlistentry id="mtm-logrep-receiver">
        <term>mtm-logrep-receiver</term>
        <listitem>
        <para>
          Receives logical replication stream from a given peer node. During recovery,
          all received transactions are applied by <literal>mtm-logrep-receiver</literal>.
          During normal operation, <literal>mtm-logrep-receiver</literal> passes transactions to
          the pool of dynamic workers (see <xref linkend="mtm-logrep-receiver-dynworker"/>).
          The number of <literal>mtm-logrep-receiver</literal> workers on each node corresponds
          to the number of peer nodes available.
        </para>
        </listitem>
      </varlistentry>
      <varlistentry id="mtm-dmq-receiver">
        <term>mtm-dmq-receiver</term>
        <listitem>
        <para>
          Receives acknowledgment for transactions sent to peers and
          checks for heartbeat timeouts.
          The number of <literal>mtm-logrep-receiver</literal> workers on each node corresponds
          to the number of peer nodes available.
        </para>
        </listitem>
      </varlistentry>
      <varlistentry id="mtm-dmq-sender">
        <term>mtm-dmq-sender</term>
        <listitem>
        <para>
          Collects acknowledgment for transactions applied on the current node and
          sends them to the corresponding <xref linkend="mtm-dmq-receiver"/> on the peer node.
          There is a single worker per <productname>PostgreSQL</productname> instance.
        </para>
        </listitem>
      </varlistentry>
      <varlistentry id="mtm-logrep-receiver-dynworker">
        <term>mtm-logrep-receiver-dynworker</term>
        <listitem>
        <para>
          Dynamic pool worker for a given <xref linkend="mtm-logrep-receiver"/>. Applies
          the replicated transaction received during normal operation. There are up to
          <literal>multimaster.max_workers</literal> workers per each peer node.
        </para>
        </listitem>
      </varlistentry>
      <varlistentry id="mtm-resolver">
        <term>mtm-resolver</term>
        <listitem>
          <para>
	    Performs Paxos to resolve unfinished transactions.
            This worker is only active during recovery or when connection with other nodes was lost.
	    There is a single worker per <productname>PostgreSQL</productname> instance.
        </para>
        </listitem>
      </varlistentry>
      <varlistentry id="mtm-campaigner">
        <term>mtm-campaigner</term>
        <listitem>
          <para>
	    Ballots for new generations to exclude some node(s) or add myself.
	    There is a single worker per <productname>PostgreSQL</productname> instance.
        </para>
        </listitem>
      </varlistentry>
      <varlistentry id="mtm-replier">
        <term>mtm-replier</term>
        <listitem>
          <para>
	    Responds to requests of <xref linkend="mtm-campaigner"/> and <xref linkend="mtm-resolver"/>.
        </para>
        </listitem>
      </varlistentry>

    </variablelist>
  </sect3>

</sect2>
  <sect2 id="multimaster-installation">
    <title>Installation and Setup</title>
    <para>
        To use <filename>multimaster</filename>, you need to install
        <productname>Postgres Pro</productname> or
        <productname>PostgreSQL</productname> on all nodes of your cluster.
        <productname>Postgres Pro</productname> includes all the required
        dependencies and extensions. For <productname>PostgreSQL</productname>
	follow build and install instructions at <ulink url="https://github.com/postgrespro/mmts#readme">readme.md</ulink>.
      </para>
  <sect3 id="multimaster-setting-up-a-multi-master-cluster">
    <title>Setting up a Multi-Master Cluster</title>
      <para>Suppose you are setting up a cluster of three nodes, with
        <literal>node1</literal>, <literal>node2</literal>, and
        <literal>node3</literal> host names. After installing <productname>PostgreSQL</productname> on all nodes, you need to
        initialize data directory on each node, as explained in <ulink url="https://postgrespro.com/docs/postgresql/current/creating-cluster"> Creating a Database Cluster</ulink>.
        If you would like to set up a multi-master cluster for an already existing <structname>mydb</structname> database,
        you can load data from <structname>mydb</structname> to one of the nodes once the cluster is initialized,
        or you can load data to all new nodes before cluster initialization using any convenient mechanism,
        such as <application>pg_basebackup</application> or <application>pg_dump</application>.
      </para>
      <para>Once the data directory is set up, complete the following steps on each
      cluster node:
    </para>
    <orderedlist>
      <listitem>
        <para>
          Modify the <filename>postgresql.conf</filename> configuration
          file, as follows:
        </para>
        <itemizedlist>
          <listitem>
              <para>Add <literal>multimaster</literal> to the <varname>shared_preload_libraries</varname> variable:</para>
              <programlisting>
shared_preload_libraries = 'multimaster'
</programlisting>
              <tip>
		<para>If the <varname>shared_preload_libraries</varname> variable is already
		defined in <filename>postgresql.auto.conf</filename>, you will need to modify
		its value using the <ulink url="https://postgrespro.com/docs/postgresql/current/sql-altersystem">ALTER SYSTEM</ulink> command.
		For details, see <ulink url="https://postgrespro.com/docs/postgresql/current/config-setting">Setting Parameters</ulink>.
		Note that in a multi-master cluster, the <literal>ALTER SYSTEM</literal> command only affects the configuration of the node from which it was run.
              </para>
              </tip>
          </listitem>
          <listitem>
            <para>
              Set up <productname>PostgreSQL</productname> parameters related to replication:
            <programlisting>
wal_level = logical
max_connections = 100
max_prepared_transactions = 300 # max_connections * N
max_wal_senders = 10            # at least N
max_replication_slots = 10      # at least 2N
wal_sender_timeout = 0
</programlisting>
            where <replaceable>N</replaceable> is the number of nodes in your cluster.
            </para>
            <para>
              You must change the replication level to
              <literal>logical</literal> as
              <filename>multimaster</filename> relies on logical
              replication. For a cluster of <replaceable>N</replaceable> nodes, enable at least <replaceable>N</replaceable>
              WAL sender processes and replication slots. Since
              <filename>multimaster</filename> implicitly adds a
              <literal>PREPARE</literal> phase to each
              <literal>COMMIT</literal> transaction, make sure to set
              the number of prepared transactions to <replaceable>N</replaceable> * <varname>max_connections</varname>.
              <varname>wal_sender_timeout</varname> should be disabled as <application>multimaster</application> uses
              its custom logic for failure detection.
            </para>
          </listitem>
          <listitem>
            <para>
              Make sure you have enough background workers allocated for
              each node:
            </para>
            <programlisting>
max_worker_processes = 250 # (N - 1) * (multimaster.max_workers + 1) + 5
</programlisting>
            <para>
              For example, for a three-node cluster with
              <literal>multimaster.max_workers</literal> = 100,
              <filename>multimaster</filename> may need up to 207
              background workers at peak times: five always-on workers
              (monitor, resolver, dmq-sender, campaigner, replier), one walreceiver
	      per each peer node and up to 200 replication dynamic workers.
	      When setting this parameter, remember
              that other modules may also use background workers at the
              same time.
            </para>
          </listitem>
          <listitem>
            <para>
              Depending on your network environment and usage patterns, you
              may want to tune other <filename>multimaster</filename>
              parameters. For details, see
              <xref linkend="multimaster-tuning-configuration-parameters"/>.
            </para>
          </listitem>
        </itemizedlist>
      </listitem>
      <listitem>
        <para>
          Start <productname>PostgreSQL</productname> on all nodes.
        </para>
      </listitem>

      <listitem>
        <para>
          Create database <structname>mydb</structname> and user <literal>mtmuser</literal>
          on each node:
        <programlisting>
CREATE USER mtmuser WITH SUPERUSER PASSWORD 'mtmuserpassword';
CREATE DATABASE mydb OWNER mtmuser;
</programlisting>
          If you are using password-based authentication, you may want to
          create a <ulink url="https://postgrespro.com/docs/postgresql/current/libpq-pgpass">password file</ulink>.
        </para>
        <para>
          You can omit this step if you already have a database you are going
          to replicate, but you are recommended to create a separate superuser
          for multi-master replication. The examples below assume that you are going to
          replicate the <structname>mydb</structname> database on behalf of
          <literal>mtmuser</literal>.
        </para>
      </listitem>

      <listitem>
        <para>
          Allow replication of the <literal>mydb</literal> database
          to each cluster node on behalf of <literal>mtmuser</literal>,
          as explained in <ulink url="https://postgrespro.com/docs/postgresql/current/auth-pg-hba-conf.html">pg_hba.conf</ulink>.
          Make sure to use the
          <ulink url="https://postgrespro.com/docs/postgresql/current/auth-methods">authentication method</ulink> that
          satisfies your security requirements. For example,
          <filename>pg_hba.conf</filename> might have the following lines on <literal>node1</literal>:
          <programlisting>
host replication mtmuser node2 md5
host mydb mtmuser node2 md5
host replication mtmuser node3 md5
host mydb mtmuser node3 md5
</programlisting>
        </para>
      </listitem>

      <listitem>
    <para>
          Connect to any node on behalf of the <literal>mtmuser</literal> database user,
          create the <filename>multimaster</filename> extension
          in the <literal>mydb</literal> database and run
          <literal>mtm.init_cluster()</literal>, specifying the connection
          string to the current node as the first argument and an array
          of connection strings to the other nodes as the second argument.
    </para>
    <para>
          For example, if you would like to connect to <literal>node1</literal>, run:
          <programlisting>
CREATE EXTENSION multimaster;
SELECT mtm.init_cluster('dbname=mydb user=mtmuser host=node1',
'{"dbname=mydb user=mtmuser host=node2", "dbname=mydb user=mtmuser host=node3"}');
</programlisting>
    </para>
    </listitem>
    <listitem>
    <para>
      To ensure that <filename>multimaster</filename> is enabled, you can run
      the <structname>mtm.status()</structname> and <structname>mtm.nodes()</structname> functions:
    </para>
    <programlisting>
SELECT * FROM mtm.status();
SELECT * FROM mtm.nodes();
</programlisting>
    <para>
      If <literal>status</literal> is equal to <literal>online</literal>
      and all nodes are present in the <structname>mtm.nodes</structname> output,
      your cluster is successfully configured and ready to use.
    </para>
    </listitem>
    </orderedlist>

  <tip>
   <para>If you have any data that must be present on one of the nodes only,
   you can exclude a particular table from replication, as follows:
<programlisting>SELECT mtm.make_table_local('table_name') </programlisting>
   </para>
  </tip>

  </sect3>
    <sect3 id="multimaster-tuning-configuration-parameters">
    <title>Tuning Configuration Parameters</title>
    <para>
      While you can use <filename>multimaster</filename> in the default
      configuration, you may want to tune several parameters for faster
      failure detection or more reliable automatic recovery.
    </para>
    <sect4 id="multimaster-setting-timeout-for-failure-detection">
      <title>Setting Timeout for Failure Detection</title>
      <para>
        To check availability of the peer nodes,
        <filename>multimaster</filename> periodically sends heartbeat
        packets to all nodes. You can define the timeout for failure detection with the following variables:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            The <literal>multimaster.heartbeat_send_timeout</literal>
            variable defines the time interval between the
            heartbeats. By default, this variable is set to 200ms.
          </para>
        </listitem>
        <listitem>
          <para>
            The <literal>multimaster.heartbeat_recv_timeout</literal>
            variable sets the timeout for the response. If no heartbeats are
            received during this time, the node is assumed to be
            disconnected and is excluded from the cluster. By default,
            this variable is set to 2000ms.
          </para>
        </listitem>
      </itemizedlist>
      <para>
        It's a good idea to set
        <literal>multimaster.heartbeat_send_timeout</literal> based on
        typical ping latencies between the nodes. Small recv/send ratio
        decreases the time of failure detection, but increases the
        probability of false-positive failure detection. When setting
        this parameter, take into account the typical packet loss ratio
        between your cluster nodes.
      </para>
    </sect4>
  </sect3>
  <sect3 id="setting-up-a-referee">
    <title>2+1 Mode: Setting up a Standalone Referee Node</title>
    <para>
      By default, <filename>multimaster</filename> uses a majority-based
      algorithm to determine whether the cluster nodes have a quorum: a cluster
      can only continue working if the majority of its nodes are alive and can
      access each other. Majority-based approach is pointless for two nodes
      cluster: if one of them fails, another one becomes inaccessible. There is
      a special 2+1 or referee mode which trades less hardware resources by
      decreasing availability: two nodes hold full copy of data, and separate
      referee node participates only in voting, acting as a tie-breaker.
    </para>
    <para>
      If one node goes down, another one requests referee grant (elects
      referee-approved generation with single node). One the grant is received,
      it continues to work normally. If offline node gets up, it recovers and
      elects full generation containing both nodes, essentially removing the
      grant - this allows the node to get it in its turn later. While the grant is
      issued, it can't be given to another node until full generation is elected
      and excluded node recovers. This ensures data loss doesn't happen by the
      price of availability: in this setup two nodes (one normal and one referee)
      can be alive but cluster might be still unavailable if the referee winner
      is down, which is impossible with classic three nodes configuration.
    </para>
    <para>
      The referee node does not store any cluster data, so it is not
      resource-intensive and can be configured on virtually any system with
      <productname>PostgreSQL</productname> installed.
    </para>
    <para>
      To avoid split-brain problems, you must have only a single referee in your
      cluster.
    </para>
    <para>
      To set up a referee for your cluster:
<orderedlist>
  <listitem>
    <para>
      Install <productname>PostgreSQL</productname> on the node you are
      going to make a referee and create the <filename>referee</filename>
      extension:
      <programlisting>
CREATE EXTENSION referee;
</programlisting>
    </para>
  </listitem>
  <listitem>
    <para>
      Make sure the <filename>pg_hba.conf</filename> file allows
      access to the referee node.
    </para>
  </listitem>
  <listitem>
    <para>
     Set up the nodes that will hold cluster data following the instructions in
     <xref linkend="multimaster-setting-up-a-multi-master-cluster"/>.
    </para>
  </listitem>
  <listitem>
    <para>
     On all data nodes, specify the referee connection string
     in the <filename>postgresql.conf</filename> file:
      <programlisting>
multimaster.referee_connstring = <replaceable>connstring</replaceable>
</programlisting>
where <replaceable>connstring</replaceable> holds <ulink url="https://postgrespro.com/docs/postgresql/current/libpq-paramkeywords">libpq options</ulink>
required to access the referee.
    </para>
  </listitem>
</orderedlist>
</para>

    <para>
      The first subset of nodes that gets connected to the referee
      wins the voting and starts working. The other nodes have to
      go through the recovery process to catch up with them
      and join the cluster. Under heavy load, the recovery can take
      unpredictably long, so it is recommended to wait for all data
      nodes going online before switching on the load when setting
      up a new cluster.
      Once all the nodes get online, the referee discards the voting
      result, and all data nodes start operating together.
    </para>

    <para>
      In case of any failure, the voting mechanism is triggered again.
      At this time, all nodes appear to be offline for a short period
      of time to allow the referee to choose a new winner, so you can
      see the following error message when trying to access the cluster:
      <literal>[multimaster] node is not online:
      current status is "disabled"</literal>.
     </para>

  </sect3>

  </sect2>
  <sect2 id="multimaster-administration"><title>Multi-Master Cluster Administration</title>
  <itemizedlist>
    <listitem>
      <para>
        <link linkend="multimaster-monitoring-cluster-status">Monitoring the Cluster Status</link>
      </para>
    </listitem>
    <listitem>
      <para>
        <link linkend="multimaster-accessing-disabled-nodes">Accessing Disabled Nodes</link>
      </para>
    </listitem>
    <listitem>
      <para>
        <link linkend="multimaster-adding-new-nodes-to-the-cluster">Adding New Nodes
        to the Cluster</link>
      </para>
    </listitem>
    <listitem>
      <para>
        <link linkend="multimaster-removing-nodes-from-the-cluster">Removing Nodes
        from the Cluster</link>
      </para>
    </listitem>
    <listitem>
      <para>
        <link linkend="multimaster-checking-data-consistency">Checking Data Consistency Across Cluster Nodes</link>
      </para>
    </listitem>
  </itemizedlist>
  <sect3 id="multimaster-monitoring-cluster-status">
    <title>Monitoring Cluster Status</title>
    <para>
      <filename>multimaster</filename> provides several functions to check the
      current cluster state.
    </para>
    <para>
      To check node-specific information, use <literal>mtm.status()</literal>:
    </para>
    <programlisting>
SELECT * FROM mtm.status();
</programlisting>
      <para>To get the list of all nodes in the cluster together with their status,
      use <literal>mtm.nodes()</literal>:
    </para>
    <programlisting>
SELECT * FROM mtm.nodes();
</programlisting>
      <para>For details on all the returned information, see <xref linkend="multimaster-functions"/>.
    </para>
  </sect3>
  <sect3 id="multimaster-accessing-disabled-nodes">
    <title>Accessing Disabled Nodes</title>
    <para>
      If a cluster node is disabled, any attempt to read or write data on
      this node raises an error by default. If you need to access the data
      on a disabled node, you can override this behavior at connection time by setting the
      <ulink url="https://postgrespro.com/docs/postgresql/current/runtime-config-logging#GUC-APPLICATION-NAME">application_name</ulink>
      parameter to
      <literal>mtm_admin</literal>. In this case, you can run read and
      write queries on this node without <application>multimaster</application>
      supervision.
    </para>
  </sect3>
  <sect3 id="multimaster-adding-new-nodes-to-the-cluster">
    <title>Adding New Nodes to the Cluster</title>
    <para>With the <filename>multimaster</filename> extension, you can add or
    drop cluster nodes. Before adding node, stop the load and ensure (with
    <literal>mtm.status()</literal>) that all nodes are <literal>online</literal>.
      When adding a new node, you need to load all the data to this node using
      <application>pg_basebackup</application> from any cluster node, and then start this node.
    </para>
    <para>
      Suppose we have a working cluster of three nodes, with
      <literal>node1</literal>, <literal>node2</literal>, and
      <literal>node3</literal> host names. To add
      <literal>node4</literal>, follow these steps:
    </para>
    <orderedlist>
      <listitem>
        <para>
          Figure out the required connection string to
          access the new node. For example, for the database
          <literal>mydb</literal>, user <literal>mtmuser</literal>, and
          the new node <literal>node4</literal>, the connection string
          can be <literal>&quot;dbname=mydb user=mtmuser host=node4&quot;</literal>.
        </para>
      </listitem>
      <listitem>
        <para>
          In <literal>psql</literal> connected to any alive node, run:
        </para>
        <programlisting>
SELECT mtm.add_node('dbname=mydb user=mtmuser host=node4');
</programlisting>
        <para>
          This command changes the cluster configuration on all nodes
          and creates replication slots for the new node. It also returns
          <literal>node_id</literal> of the new node, which will be required
          to complete the setup.
        </para>
      </listitem>
      <listitem>
        <para>
          Go to the new node and clone all the data from one of the alive nodes to this node:
        </para>
        <programlisting>
pg_basebackup -D <replaceable>datadir</replaceable> -h node1 -U mtmuser -c fast -v
</programlisting>
        <para>
          <application>pg_basebackup</application> copies the entire data
          directory from <literal>node1</literal>, together with
          configuration settings, and prints the last LSN replayed from WAL,
          such as <literal>'0/12D357F0'</literal>.
          This value will be required to complete the setup.
        </para>
      </listitem>
      <listitem>
        <para>
          Configure the new node to boot with <literal>recovery_target=immediate</literal> to prevent redo
          past the point where replication will begin. Add to <filename>postgresql.conf</filename>:
        </para>
          <programlisting>
restore_command = 'false'
recovery_target = 'immediate'
recovery_target_action = 'promote'
          </programlisting>
          <para>
            And create <literal>recovery.signal</literal> file in the data directory.
          </para>
      </listitem>
      <listitem>
        <para>
          Start <productname>PostgreSQL</productname> on the new node.
        </para>
      </listitem>
      <listitem>
        <para>
          In <literal>psql</literal> connected to the node used to take the base backup, run:
        <programlisting>
SELECT mtm.join_node(4, '0/12D357F0');
</programlisting>
          where <literal>4</literal> is the <literal>node_id</literal> returned
          by the <literal>mtm.add_node()</literal> function call and <literal>'0/12D357F0'</literal>
          is the LSN value returned by <application>pg_basebackup</application>.
        </para>
      </listitem>
    </orderedlist>

  </sect3>

  <sect3 id="multimaster-removing-nodes-from-the-cluster">
    <title>Removing Nodes from the Cluster</title>
    <para>
      Before removing node, stop the load and ensure (with
    <literal>mtm.status()</literal>) that all nodes (except the ones to be
    dropped) are <literal>online</literal>. Shut down the nodes you are going to remove.
      To remove the node from the cluster:
    </para>
    <orderedlist>
     <listitem>
      <para>
        Run the <literal>mtm.nodes()</literal> function to learn the ID of the node to be removed:
        <programlisting>
SELECT * FROM mtm.nodes();
</programlisting>
      </para>
    </listitem>
     <listitem>
      <para>
        Run the <literal>mtm.drop_node()</literal> function with
        this node ID as a parameter:
      </para>
    <programlisting>
SELECT mtm.drop_node(3);
</programlisting>
     <para>
      This will delete replication slots for node 3 on all cluster nodes and stop replication to
      this node.
     </para>
    </listitem>
    </orderedlist>
    <para>If you would like to return the node to the cluster later, you will have to add it
      as a new node, as explained in <xref linkend="multimaster-adding-new-nodes-to-the-cluster"/>.
    </para>

  </sect3>

  <sect3 id="uninstalling-multimaster-extension">
   <title>Uninstalling the multimaster Extension</title>

  <para>
   If you would like to continue using the node that has been removed
   from the cluster in the standalone mode, you have to drop the
   <filename>multimaster</filename> extension on this node and
   clean up all <filename>multimaster</filename>-related subscriptions
   and uncommitted transactions to ensure that the node is no longer
   associated with the cluster.
  </para>

  <procedure>
   <step>
    <para>
      Remove <literal>multimaster</literal> from
      <ulink url="https://postgrespro.com/docs/postgresql/current/runtime-config-client#GUC-SHARED-PRELOAD-LIBRARIES">shared_preload_libraries</ulink>
      and restart
     <productname>PostgreSQL</productname>.
    </para>
   </step>
   <step>
    <para>
     Delete the <filename>multimaster</filename> extension and publication:
     <programlisting>
DROP EXTENSION multimaster;
DROP PUBLICATION multimaster;
</programlisting>
    </para>
   </step>
   <step>
    <para>
     Review the list of existing subscriptions using the
     <literal>\dRs</literal> command and delete each subscription
     that starts with the <literal>mtm_sub_</literal> prefix:
     <programlisting>
\dRs
DROP SUBSCRIPTION mtm_sub_<replaceable>subscription_name</replaceable>;
</programlisting>
    </para>
   </step>
   <step>
    <para>
     Review the list of existing replication slots and delete
     each slot that starts with the <literal>mtm_</literal> prefix:
     <programlisting>
SELECT * FROM pg_replication_slots;
SELECT pg_drop_replication_slot('mtm_<replaceable>slot_name</replaceable>');
</programlisting>
    </para>
   </step>
   <step>
    <para>
     Review the list of existing replication origins and delete
     each origin that starts with the <literal>mtm_</literal> prefix:
     <programlisting>
SELECT * FROM pg_replication_origin;
SELECT pg_replication_origin_drop('mtm_<replaceable>origin_name</replaceable>');
</programlisting>
    </para>
   </step>
   <step>
    <para>
     Review the list of prepared transaction left, if any:
     <programlisting>
SELECT * FROM pg_prepared_xacts;
</programlisting>
     You have to commit or abort these transactions by running
     <literal>ABORT PREPARED <replaceable>transaction_id</replaceable></literal> or
     <literal>COMMIT PREPARED <replaceable>transaction_id</replaceable></literal>, respectively.
    </para>
   </step>
  </procedure>

   <para>
    Once all these steps are complete, you can start using
    the node in the standalone mode, if required.
   </para>
  </sect3>

  <sect3 id="multimaster-checking-data-consistency">
   <title>Checking Data Consistency Across Cluster Nodes</title>

   <para>
    You can check that the data is the same on all cluster nodes
    using the <xref linkend="mtm-check-query"/> function.
   </para>

   <para>
    As a parameter, this function takes the text of a query you would like to
    run for data comparison. When you call this function, it takes a consistent
    snapshot of data on each cluster node and runs this query against the
    captured snapshots. The query results are compared between pairs of nodes.
    If there are no differences, this function returns <literal>true</literal>.
    Otherwise, it reports the first detected difference in a warning
    and returns <literal>false</literal>.
   </para>

   <para>
    To avoid false-positive results, always use the <literal>ORDER BY</literal>
    clause in your test query. For example, suppose you would like to check
    that the data in a <structname>my_table</structname> is the same on all
    cluster nodes. Compare the results of the following queries:

<programlisting>
postgres=# SELECT mtm.check_query('SELECT * FROM my_table ORDER BY id');
 check_query
-------------
 t
(1 row)
</programlisting>

<programlisting>
postgres=# SELECT mtm.check_query('SELECT * FROM my_table');
WARNING: mismatch in column 'b' of row 0: 256 on node0, 255 on node1
 check_query
-------------
 f
(1 row)
</programlisting>
    Even though the data is the same, the second query reports an issue
    because the order of the returned data differs between cluster nodes.
   </para>

  </sect3>


  </sect2>
  <sect2 id="multimaster-reference"><title>Reference</title>
<sect3 id="multimaster-guc-variables">
  <title>Configuration Parameters</title>
<variablelist>


  <varlistentry><term><varname>multimaster.heartbeat_recv_timeout</varname><indexterm><primary><varname>multimaster.heartbeat_recv_timeout</varname></primary></indexterm></term><listitem><para>
    Timeout, in
    milliseconds. If no heartbeat message is received from the node
    within this timeframe, the node is excluded from the cluster.
    </para><para>Default: 2000 ms
  </para></listitem></varlistentry>
  <varlistentry><term><varname>multimaster.heartbeat_send_timeout</varname><indexterm><primary><varname>multimaster.heartbeat_send_timeout</varname></primary></indexterm></term><listitem><para>
    Time interval
    between heartbeat messages, in milliseconds. An arbiter process
    broadcasts heartbeat messages to all nodes to detect connection
    problems. </para><para>Default: 200 ms
  </para></listitem></varlistentry>

  <varlistentry>
    <term><varname>multimaster.max_workers</varname>
      <indexterm><primary><varname>multimaster.max_workers</varname></primary>
      </indexterm>
    </term>
    <listitem>
      <para>The maximum number of <literal>walreceiver</literal> workers per peer node.
      </para>
      <important>
      <para>This parameter should be used with caution. If the number of simultaneous transactions
      in the whole cluster is bigger than the provided value, it can lead to undetected deadlocks.
      </para>
      </important>
      <para>Default: 100
      </para>
    </listitem>
  </varlistentry>
  <varlistentry id="mtm-monotonic-sequences">
    <term><varname>multimaster.monotonic_sequences</varname>
      <indexterm><primary><varname>multimaster.monotonic_sequences</varname></primary>
      </indexterm>
    </term>
    <listitem>
      <para>Defines the sequence generation mode for unique identifiers. This variable can
      take the following values:
      <itemizedlist>
      <listitem>
      <para>
      <literal>false</literal> (default) &mdash;
      ID generation on each node is started with this node number and is incremented by
      the number of nodes. For example, in a three-node cluster, 1, 4, and 7 IDs are allocated to the objects written onto
      the first node, while 2, 5, and 8 IDs are reserved for the second node. If you
      change the number of nodes in the cluster, the incrementation interval for new IDs is adjusted accordingly.
      </para>
      </listitem>
      <listitem>
      <para>
      <literal>true</literal> &mdash;
      the generated sequence increases monotonically cluster-wide.
      ID generation on each node is started with this node number and is incremented by
      the number of nodes, but the values are omitted if they are smaller than the already generated IDs on another node.
      For example, in a three-node cluster, if 1, 4 and 7 IDs are already allocated to the objects on
      the first node, 2 and 5 IDs will be omitted on the second node. In this case, the first ID on the second node is 8.
      Thus, the next generated ID is always higher than the previous one, regardless of the cluster node.
      </para>
      </listitem>
      </itemizedlist>
      </para>
      <para>Default: <literal>false</literal>
      </para>
    </listitem>
  </varlistentry>
  <varlistentry id="mtm-referee-connstring" xreflabel="multimaster.referee_connstring">
    <term><varname>multimaster.referee_connstring</varname>
      <indexterm><primary><varname>multimaster.referee_connstring</varname></primary>
      </indexterm>
    </term>
    <listitem>
      <para>Connection string to access the referee node. You must set this parameter
      on all cluster nodes if the referee is set up.
      </para>
    </listitem>
  </varlistentry>
  <varlistentry id="mtm-remote-functions">
    <term><varname>multimaster.remote_functions</varname>
      <indexterm><primary><varname>multimaster.remote_functions</varname></primary>
      </indexterm>
    </term>
    <listitem>
      <para>Provides a comma-separated list of function names that should be executed
      remotely on all multimaster nodes instead of replicating the result of their work.
      </para>
    </listitem>
  </varlistentry>
  <varlistentry>
    <term><varname>multimaster.trans_spill_threshold</varname>
      <indexterm><primary><varname>multimaster.trans_spill_threshold</varname></primary>
      </indexterm>
    </term>
    <listitem>
      <para>The maximal size of transaction, in kB. When this threshold is reached, the transaction is written to the disk.
      </para>
      <para>Default: 100MB
      </para>
    </listitem>
  </varlistentry>

    <varlistentry id="mtm-break-connection">
      <term><varname>multimaster.break_connection</varname>
      <indexterm><primary><varname>multimaster.break_connection</varname></primary></indexterm>
      </term>
      <listitem>
        <para>Break connection with clients connected to the node if this node disconnects
        from the cluster. If this variable is set to <literal>false</literal>, the client stays
        connected to the node but receives an error that the node is disabled.
        </para>
        <para>Default: <literal>false</literal></para>
      </listitem>
    </varlistentry>
</variablelist>
</sect3>

  <sect3 id="multimaster-functions"><title>Functions</title>
  <variablelist>

    <varlistentry>
     <term>
      <function>mtm.init_cluster(<parameter>my_conninfo</parameter> <type>text</type>,
                <parameter>peers_conninfo</parameter> <type>text[]</type>)</function>
      <indexterm>
       <primary><function>mtm.init_cluster</function></primary>
      </indexterm>
     </term>
     <listitem>
      <para>
      Initializes cluster configuration on all nodes. It connects the
      current node to all nodes listed in <parameter>peers_conninfo</parameter>
      and creates the <application>multimaster</application> extension,
      replications slots, and replication origins on each node. Run this function
      once all the nodes are running and can accept connections.
      </para>
      <para>
       Arguments:
       <itemizedlist>
        <listitem>
          <para>
            <parameter>my_conninfo</parameter> &mdash; connection string to the
            node on which you are running this function. Peer nodes use this
            string to connect back to this node.
          </para>
        </listitem>
        <listitem>
          <para>
            <parameter>peers_conninfo</parameter> &mdash; an array of connection
            strings to all the other nodes to be added to the cluster.
          </para>
        </listitem>
        </itemizedlist>
      </para>
      <para>
      </para>
     </listitem>
    </varlistentry>

    <varlistentry>
     <term>
      <function>mtm.add_node(<parameter>connstr</parameter> <type>text</type>)</function>
      <indexterm>
       <primary><function>mtm.add_node</function></primary>
      </indexterm>
     </term>
     <listitem>
      <para>Adds a new node to the cluster. This function should be called
      before loading data to this node using <application>pg_basebackup</application>.
      <function>mtm.add_node</function> creates the required replication slots for a new node,
      so you can add a node while the cluster is under load.
      </para>
      <para>
       Arguments:
       <itemizedlist>
        <listitem>
         <para>
         <parameter>connstr</parameter> &mdash; connection string for the
              new node. For example, for the database
              <literal>mydb</literal>, user <literal>mtmuser</literal>,
              and the new node <literal>node4</literal>, the connection
              string is
              <literal>&quot;dbname=mydb user=mtmuser host=node4&quot;</literal>.</para>
        </listitem>
        </itemizedlist>
      </para>
      <para>
      </para>
     </listitem>
    </varlistentry>

    <varlistentry>
     <term>
      <function>mtm.join_node(<parameter>node_id</parameter> <type>int</type>, <parameter>backup_end_lsn</parameter> <type>pg_lsn</type>)</function>
      <indexterm>
       <primary><function>mtm.join_node</function></primary>
      </indexterm>
     </term>
     <listitem>
      <para>
        Completes the cluster setup after adding a new node. This function should be called
        after the added node has been started.
      </para>
      <para>
       Arguments:
       <itemizedlist>
        <listitem>
         <para>
         <parameter>node_id</parameter> &mdash; ID of the node to add to the cluster.
          It corresponds to the value in the <literal>id</literal> column returned by <literal>mtm.nodes()</literal>.
          </para>
          <para>
          <parameter>backup_end_lsn</parameter> &mdash; the last LSN of the base backup
          copied to the new node. This LSN will be used as the starting point for data
          replication once the node joins the cluster.
          </para>
        </listitem>
        </itemizedlist>
      </para>
      <para>
      </para>
     </listitem>
    </varlistentry>

    <varlistentry>
     <term>
      <function>mtm.drop_node(<parameter>node_id</parameter> <type>integer</type>)</function>
      <indexterm>
       <primary><function>mtm.drop_node</function></primary>
      </indexterm>
     </term>
     <listitem>
      <para>Excludes a node from the cluster.
      </para>
      <para>
       If you would like to continue using this node outside of
       the cluster in the standalone mode, you have to uninstall the
       <filename>multimaster</filename> extension from this node,
       as explained in <xref linkend="uninstalling-multimaster-extension"/>.
      </para>
      <para>
       Arguments:
       <itemizedlist>
        <listitem>
         <para>
         <parameter>node_id</parameter> &mdash; ID of the node being dropped.
          It corresponds to the value in the <literal>id</literal> column returned by <literal>mtm.nodes()</literal>.
          </para>
        </listitem>
        </itemizedlist>
      </para>
      <para>
      </para>
     </listitem>
    </varlistentry>

    <varlistentry>
     <term>
      <function>mtm.alter_sequences()</function>
      <indexterm>
       <primary><function>mtm.alter_sequences</function></primary>
      </indexterm>
     </term>
     <listitem>
      <para>Fixes unique identifiers on all cluster nodes.
      This may be required after restoring all nodes from a single
      base backup.
      </para>
     </listitem>
    </varlistentry>

    <varlistentry id="mtm-get-cluster-state">
     <term>
      <function>mtm.status()</function>
      <indexterm>
       <primary><function>mtm.status()</function></primary>
      </indexterm>
     </term>
     <listitem>
      <para>Shows the status of the <filename>multimaster</filename> extension on the current node. Returns a tuple of the following values:
      </para>
       <itemizedlist>
          <listitem>
            <para>
              <parameter>my_node_id</parameter>, <type>int</type> &mdash; ID of this node.
            </para>
          </listitem>
          <listitem>
            <para>
              <parameter>status</parameter>, <type>text</type> &mdash; status of the node. Possible values are:
	      <literal>online</literal>, <literal>recovery</literal>, <literal>catchup</literal>, <literal>disabled</literal> (need to recover, but not yet clear from whom), <literal>isolated</literal> (online in current generation, but some members are disconnected).
            </para>
          </listitem>
          <listitem>
            <para>
              <parameter>connected</parameter>, <type>int[]</type> &mdash; array of peer IDs connected to this node.
            </para>
          </listitem>
          <listitem>
            <para>
              <parameter>gen_num</parameter>, <type>int8</type> &mdash; current generation number.
            </para>
          </listitem>
          <listitem>
            <para>
              <parameter>gen_members</parameter>, <type>int[]</type> &mdash; array of current generation members node IDs.
            </para>
          </listitem>
	  <listitem>
            <para>
              <parameter>gen_members_online</parameter>, <type>int[]</type> &mdash; array of current generation members node IDs which are <literal>online</literal> in it.
            </para>
          </listitem>
	  <listitem>
	    <para>
	      <parameter>gen_configured</parameter>, <type>int[]</type> &mdash; array of node IDs configured in current generation.
	    </para>
	  </listitem>
        </itemizedlist>
     </listitem>
    </varlistentry>

    <varlistentry>
     <term>
      <function>mtm.nodes()</function>
      <indexterm>
       <primary><function>mtm.nodes()</function></primary>
      </indexterm>
     </term>
     <listitem>
      <para>Shows the information on all nodes in the cluster. Returns a tuple of the following values:
      </para>
      <para>
	<itemizedlist>
          <listitem>
            <para>
              <parameter>id</parameter>, <type>integer</type> &mdash; node ID.
            </para>
          </listitem>
	  <listitem>
            <para>
              <parameter>conninfo</parameter>, <type>text</type> &mdash; connection string to this node.
            </para>
          </listitem>
	  <listitem>
            <para>
              <parameter>is_self</parameter>, <type>boolean</type> &mdash; is it me?
            </para>
          </listitem>

          <listitem>
            <para>
              <parameter>enabled</parameter>, <type>boolean</type> &mdash; is this node online in current generation?
            </para>
          </listitem>
          <listitem>
            <para>
              <parameter>connected</parameter>, <type>boolean</type> &mdash; shows whether the node is connected to our node.
            </para>
          </listitem>

          <listitem>
            <para>
              <parameter>sender_pid</parameter>, <type>integer</type> &mdash; WAL sender process ID.
            </para>
          </listitem>

          <listitem>
            <para>
              <parameter>receiver_pid</parameter>, <type>integer</type> &mdash; WAL receiver process ID.
            </para>
          </listitem>

	  <listitem>
            <para>
              <parameter>n_workers</parameter>, <type>text</type> &mdash; number of started dynamic apply workers from this node.
            </para>
          </listitem>

          <listitem>
            <para>
              <parameter>receiver_mode</parameter>, <type>text</type> &mdash; in which mode receiver from this node works.
              Possible values are: <literal>disabled</literal>, <literal>recovery</literal>, <literal>normal</literal>.
            </para>
          </listitem>
        </itemizedlist>
      </para>
     </listitem>
    </varlistentry>

    <varlistentry>
     <term>
      <function>mtm.make_table_local(<parameter>relation</parameter> <type>regclass</type>)</function>
      <indexterm>
       <primary><function>mtm.make_table_local</function></primary>
      </indexterm>
     </term>
     <listitem>
      <para>Stops replication for the specified table.
      </para>
      <para>
       Arguments:
       <itemizedlist>
        <listitem>
         <para>
         <parameter>relation</parameter> &mdash; the table you would like to
              exclude from the replication scheme.</para>
        </listitem>
        </itemizedlist>
      </para>
      <para>
      </para>
     </listitem>
    </varlistentry>

    <varlistentry id="mtm-check-query" xreflabel="mtm.check_query(query_text)">
      <term><function>mtm.check_query(<parameter>query_text</parameter> <type>text</type>)</function>
      <indexterm><primary><function>mtm.check_query</function></primary></indexterm>
      </term>
      <listitem>
        <para>
         Checks data consistency across cluster nodes. This function takes a
         snapshot of the current state of each node, runs the specified query
         against these snapshots, and compares the results. If the results are
         different between any two nodes, displays a warning with the first
         found issue and returns <literal>false</literal>.
         Otherwise, returns <literal>true</literal>.
        </para>
        <para>
       Arguments:
       <itemizedlist>
        <listitem>
         <para>
         <parameter>query_text</parameter> &mdash; the query you would like to
         run on all nodes for data comparison. To avoid false-positive results,
         always use the <literal>ORDER BY</literal> clause in the test query.
         </para>
        </listitem>
        </itemizedlist>
        </para>
      </listitem>
    </varlistentry>

    <varlistentry id="mtm-get-snapshots">
      <term><function>mtm.get_snapshots()</function>
      <indexterm><primary><function>mtm.get_snapshots</function></primary></indexterm>
      </term>
      <listitem>
        <para>
         Takes a snapshot of data on each cluster node and returns the
         snapshot ID. The snapshots remain available until the
         <function>mtm.free_snapshots()</function> is called, or the current
         session is terminated. This function is used by the
         <xref linkend="mtm-check-query"/>, there is no need to call
         it manually.
        </para>
      </listitem>
    </varlistentry>

    <varlistentry id="mtm-free-snapshots">
      <term><function>mtm.free_snapshots()</function>
      <indexterm><primary><function>mtm.free_snapshots</function></primary></indexterm>
      </term>
      <listitem>
        <para>
         Removes data snapshots taken by the
         <function>mtm.get_snapshots()</function> function.
         This function is used by the <xref linkend="mtm-check-query"/>,
         there is no need to call it manually.
        </para>
      </listitem>
    </varlistentry>

    </variablelist>
  </sect3>

  </sect2>
  <sect2 id="multimaster-compatibility">
    <title>Compatibility</title>

    <sect3 id="multimaster-local-ddl">
    <title>Local and Global DDL Statements</title>
    <para>
      By default, any DDL statement is executed on all cluster nodes, except
      the following statements that can only act locally on a given node:
    </para>
    <itemizedlist>
      <listitem><para><literal>ALTER SYSTEM</literal></para></listitem>
      <listitem><para><literal>CREATE DATABASE</literal></para></listitem>
      <listitem><para><literal>DROP DATABASE</literal></para></listitem>
      <listitem><para><literal>REINDEX</literal></para></listitem>
      <listitem><para><literal>CHECKPOINT</literal></para></listitem>
      <listitem><para><literal>CLUSTER</literal></para></listitem>
      <listitem><para><literal>LOAD</literal></para></listitem>
      <listitem><para><literal>LISTEN</literal></para></listitem>
      <listitem><para><literal>CHECKPOINT</literal></para></listitem>
      <listitem><para><literal>NOTIFY</literal></para></listitem>
    </itemizedlist>
    </sect3>
  </sect2>

  <sect2 id="multimaster-authors">
    <title>Authors</title>
    <para>
      Postgres Professional, Moscow, Russia.
    </para>
    <sect3 id="multimaster-credits">
      <title>Credits</title>
      <para>
        The replication mechanism is based on logical decoding and an
        earlier version of the <filename>pglogical</filename> extension
        provided for community by the 2ndQuadrant team.
      </para>
      <para>The Paxos consensus algorithm is described at:</para>
      <itemizedlist>
      <listitem>
      <para>
        Leslie Lamport. <ulink url="https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf"><citetitle>The Part-Time Parliament</citetitle></ulink>
      </para>
      </listitem>
      </itemizedlist>
      <para>Parallel replication and recovery mechanism is similar to the one described in:</para>
      <itemizedlist>
      <listitem>
      <para>
        Odorico M. Mendizabal, et al. <ulink url="https://link.springer.com/chapter/10.1007/978-3-319-14472-6_9"><citetitle>Checkpointing in Parallel State-Machine Replication</citetitle></ulink>.
      </para>
      </listitem>
      </itemizedlist>
    </sect3>
  </sect2>
</sect1>
